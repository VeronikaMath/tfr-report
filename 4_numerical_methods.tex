\section{Numerical methods}
A lot of this is also already written above.
\begin{itemize}
    \item Iterative scheme for solving sequence of PDEs (resulting in $(u^{(k)},z^{(k)})$) (This is needed here but not for some other types of problems - provide examples from PDE constrained optimisation theory?)
    \item Finite difference scheme for $u^{(k)}$
    \item Monte Carlo methods + Euler-Maruyama scheme + integral approximation for $z^{(k)}$
    \item Linear interpolation of numerical solutions (piecewise trilinear Lagrange interpolation)
    \item Motivation for mixed approach can go here! It's a novel approach which has not been done before, and could have benefits for numerical simulation - e.g. FD methods require inverting large matrices, whilst MC computations can be serialised? Also explain why only the dual equation fits into the MC framework.
\end{itemize}

In this section, we present a numerical approach for solving the optimisation problem presented in \autoref{sec:theory}, by solving the coupled PDE system we derived through a Lagrangian formulation of the optimisation problem. First, we present an iterative approach for obtaining the functions $u$ and $z$ which solve the coupled PDEs in \autoref{eq:coupled-pdes}. 

Then, we describe an approach to numerically solve the coupled PDEs of \autoref{eq:coupled-pdes} using one numerical method to solve the primal equation at each iteration, and a different numerical method to solve the dual equation. Specifically, we propose solving the primal equation using a finite difference scheme, and solving the dual equation using Monte Carlo methods. This choice of numerical methods is motivated by the fact that the dual equation fits nicely into the framework of Feynman-Kac formulae, which allow us to probabilistically represent the solution to a PDE, as well as a boundary value problem specifically, in terms of the expectation of an expression involving a stochastic process which obeys a particular SDE. Meanwhile, the primal equation takes a form which does not in general fit into the Feynman-Kac framework, so we need to apply a different type of numerical method. 

A further motivation for applying a mix of numerical methods comes from the possibility to benefit from the perks of each numerical method. Finite difference methods in general require matrix multiplication and inversion of large matrices, whose size increases quickly with the addition of more variables to the PDE, and with the refinement of the grid for the numerical solution. However, for sparse matrices highly effective methods exist for performing these operations, even for very large matrices. On the other hand, Monte Carlo methods require a large number of independent simulations of the SDE for each grid point at which we wish to know the solution to the PDE, which can take a long time to run. However, since the simulations for each grid point are fully independent, these simulations are easily parallelised, or even serialised, which speeds up the computation time significantly.

\subsection{Iterative scheme}\label{sec:iterative-scheme}

% - Overall idea of numerical methods

% - Solving coupled PDEs iteratively: feeding soln of one into the other etc

% - IC/TC and boundary conditions

% - Specifics of the finite difference scheme 

% - Specifics of the MC scheme 

% - Interpolation of numerical solutions

We recall that we want to obtain the primal $u$ and the dual $z$ which solve the coupled PDEs 
%
\begin{align} 
    % \label{eq:coupled-pdes}
    \begin{cases} 
        \partial_t u + \div{(\vecbf{b}u)} - \div{(\vecbf{A} \nabla u)} &= \frac{1}{\alpha} z\\
        \partial_t z + \vecbf{b} \cdot \nabla z + \div{(\vecbf{A} \nabla z)} &= u - d_T
    \end{cases}
\end{align}
%
with boundary conditions 
%
\begin{align}
    \begin{cases} 
        {u \rvert}_{\partial \Omega} &= 0\\
        {z \rvert}_{\partial \Omega} &= 0\\
        u(0,x,v) &= f_0(x,v)\\
        z(T,x,v) &= 0,
    \end{cases} 
\end{align}
%
where the equation for $u$ is forward in time, and the equation for $z$ is backward in time. 

Since the solution of the primal equation depends on the solution of the dual equation and vice versa, we will take an iterative approach, numerically solving the primal equation first, using that numerical solution as input into the numerical scheme for the dual equation, etc. The overall structure of the iterative procedure will be as follows:

% \begin{enumerate}
%     \item Initialize $u$ as $f_0(x,v)$ and solve the equation for $u$ with no source term, forward in time up to $t=T$.
%     \item Initialize $z$ as $z(T,x,v)=u(T,x,v)$, using the $u$ calculated in step 1, and with the calculated $u$ as source term, and solve backward in time to $t=0$.
%     \item Again initialize $u$ as $f_0(x,v)$ and solve forward in time to $t=T$, now using the $z$ calculated in step 2 as the source term.
%     \item Repeat from step 2 until $\lVert u_{prev} - u_{curr}\rVert < tol$ for some tolerance, and in some suitable norm.
% \end{enumerate}

\begin{enumerate}
    \item Consider the initial value problem
    \begin{align} 
        \begin{cases}
        &\partial_t u^{(0)} + \div{(\vecbf{b}u^{(0)})} - \div{(\vecbf{A} \nabla u^{(0)})} = 0\\
        &u^{(0)}(0,x,v) = f_0(x,v)
        \end{cases}
    \end{align}
    and obtain its solution $u^{(0)}(t,x,v)$.
    \item Then, using $u^{(0)}$ as defined above, consider the terminal value problem 
    \begin{align} 
        \begin{cases} 
            &\partial_t z^{(0)} + \vecbf{b} \cdot \nabla z^{(0)} + \div{(\vecbf{A} \nabla z^{(0)})} = u^{(0)} - d_T\\
            &z^{(0)}(T,x,v) = 0
        \end{cases}
    \end{align}
    and obtain its solution $z^{(0)}(t,x,v)$.
    \item For $k=1,\dots,k_{max}$:
    \begin{enumerate}
        \item Using $z^{(k-1)}$ as defined above, consider the initial value problem 
        \begin{align}
            \label{eq:primal-iteration} 
            \begin{cases}
                &\partial_t u^{(k)} + \div{(\vecbf{b}u^{(k)})} - \div{(\vecbf{A} \nabla u^{(k)})} = \frac{1}{\alpha} z^{(k-1)}\\
                &u^{(k)}(0,x,v) = f_0(x,v)
            \end{cases}
        \end{align}
        and obtain its solution $u^{(k)}(t,x,v)$.
        \item Then, using $u^{(k)}$ as defined above, consider the terminal value problem 
        \begin{align}
            \label{eq:dual-iteration}
            \begin{cases}
                &\partial_t z^{(k)} + \vecbf{b} \cdot \nabla z^{(k)} + \div{(\vecbf{A} \nabla z^{(k)})} = u^{(k)} - d_T\\
                &z^{(k)}(T,x,v) = 0
            \end{cases} 
        \end{align} 
        and obtain its solution.
    \end{enumerate}
    \item Repeat (a) and (b) until $k=k_{max}$ or $\lVert u^{(k-1)} - u^{(k)} \rVert < tol$, for some tolerance, and in some suitable norm. For example, me may use the $L^2([0,T],L^2(\Omega))$-norm that we chose when formulating the optimisation problem.
\end{enumerate}

When implementing this procedure numerically, we also need to numerically approximate the solutions of the specified initial and terminal value problems. As discussed, we will solve the $u^{(k)}$-equations using a finite difference scheme and the $z^{(k)}$-equations using a Monte Carlo approach. In the following sections, we describe in more detail how to implement these numerical methods, and cover the details of the numerical schemes we propose to use.

\subsection{Finite difference scheme for primal equation}

To formulate the finite difference scheme for the forward equation, we need to discretise the derivatives that feature in the equation. Recalling the form of the coefficients $\vecbf{A}$ and $\vecbf{b}$, this means that we need to discretise first order derivatives with respect to time, positiion, and velocity, and a second order derivative with respect to velocity.

We approcimate the $t$-derivative by a backward difference as
%
\begin{align} 
    \partial_t u(t,x,v) \approx \frac{u(t,x,v) - u(t-\Delta t,x,v)}{\Delta t}
\end{align}
%
and similarly the $x$-derivative as
%
\begin{align} 
    \partial_x u(t,x,v) \approx \frac{u(t,x,v) - u(t,x-\Delta x,v)}{\Delta x}
\end{align}
%
and the first order $v$-derivative as
%
\begin{align} 
    \partial_v u(t,x,v) \approx \frac{u(t,x,v) - u(t,x,v-\Delta v)}{\Delta v}.
\end{align}
%
Finally, we approximate the second order $v$-derivative by a central difference as
%
\begin{align} 
    \partial^2_v u(t,x,v) &\approx \frac{\frac{u(t,x,v+\Delta v)-u(t,x,v)}{\Delta v}-\frac{u(t,x,v)-u(t,x,v-\Delta v)}{\Delta v}}{\Delta v}\\
    &= \frac{u(t,x,v+\Delta v)- 2 u(t,x,v) + u(t,x,v-\Delta v)}{{(\Delta v)}^2}.
\end{align}
%
Now, we uniformly partition the domain, and let $\Delta t = \frac{T}{N}$,$\Delta x = \frac{X}{M}$, $\Delta v = \frac{V}{M}$, where $T$ denotes the final time, and $X$ and $V$ denote the maximum position and velocity in the domain respectively. Here, we have also chosen to include the same number of points in the position and velocity directions, but a potentially different number of points in the time direction. By denoting  $t_n=n\Delta t$, $x_i=i\Delta x$,$v_j=j\Delta v$ and $u(t_n,x_i,v_j)$ by $u^n_{i,j}$, we can write down a finite difference scheme approximating $u$. Specifically, we write down a scheme that is explicit in the first derivatives but implicit in the second derivative. First writing down this scheme having only discretised in time we have
%
\begin{align} % or should this be z^{n-1}?
    \frac{u^n-u^{n-1}}{\Delta t} + \div{(\vecbf{b}u^{n-1} - \vecbf{A} \nabla u^n)} = \frac{1}{\alpha} z^n,
\end{align}
%
where $z^n$ is a discretised solution of the backward equation, which we will describe in more detail later on.

The finite difference scheme for $u$ discretised in $t$,$x$, and $v$ is then given by 
%
\begin{align} 
    \frac{1}{\alpha}z^n_{i,j} &= \frac{u^n_{i,j}-u^{n-1}_{i,j}}{\Delta t} + v_j \frac{u^{n-1}_{i,j}-u^{n-1}_{i-1,j}}{\Delta x} - u^{n-1}_{i,j}a'(v_j) - a(v_j)\frac{u^{n-1}_{i,j}-u^{n-1}_{i,j-1}}{\Delta v}\\
     &- \frac{\sigma^2}{2}\frac{u^n_{i,j+1}-2u^n_{i,j}+u^n_{i,j-1}}{(\Delta v)^2}.
\end{align}
%
Collecting terms in $u^n$ and $u^{n-1}$ we get
%
\begin{align} 
    \label{eq:finite-difference}
    u^n_{i,j} - \frac{\sigma^2\Delta t}{2(\Delta v)^2}(u^n_{i,j+1}-2u^n_{i,j}+u^n_{i,j-1}) &= u^{n-1}_{i,j} - \frac{\Delta t}{\Delta x}v_j(u^{n-1}_{i,j}-u^{n-1}_{i-1,j}) + \Delta t a'(v_j) u^{n-1}_{i,j}\\ 
    &+ \frac{\Delta t}{\Delta v} a(v_j)(u^{n-1}_{i,j}-u^{n-1}_{i,j-1}) + \Delta t z^{n}_{i,j}.
\end{align}
%

We can write this in matrix form by letting 
%
\begin{align} 
    \vecbf{u}^n = 
    \begin{pmatrix}
        &u^n_{1,1}\\
        & u^n_{1,2}\\
        & \cdots \\
        &u^n_{1,M}\\
        &u^n_{2,1}\\
        & \cdots \\ 
        & \cdots \\ 
        &u^n_{M,M-1}\\
        &u^n_{M,M} 
    \end{pmatrix}
\end{align}
%
and similarly defining $\vecbf{u}^{n-1}$ and $\vecbf{z}^n$. The matrix form of \autoref{eq:finite-difference} then becomes 
%
\begin{align} 
    \label{eq:fd-matrix}
    \vecbf{B} \vecbf{u}^n = \vecbf{C} \vecbf{u}^{n-1} + \Delta t \vecbf{z}^n,
\end{align}
%
where $\vecbf{B}$ and $\vecbf{C}$ are matrices of size $M^2 \cross M^2$, and $\vecbf{u}^n$, $\vecbf{u}^{n-1}$ and $\vecbf{z}^n$ are column vectors of length $M^2$. Enforcing zero boundary conditions, the nonzero entries of $\vecbf{B}$ are given by
%
\begin{align} 
    \begin{cases} 
        B_{k,k} &= 1 + \sigma^2 \frac{\Delta t}{(\Delta v)^2}\\
        B_{k,k+1} &= - \frac{\sigma^2}{2} \frac{\Delta t}{(\Delta v)^2}\\ 
        B_{k,k-1} &= - \frac{\sigma^2}{2} \frac{\Delta t}{(\Delta v)^2}
    \end{cases}
\end{align}
%
and the nonzero entries of $\vecbf{C}$ are given by 
%
\begin{align} 
    \begin{cases}
        C_{k,k} &= \beta_{k \mod M}\\
        C_{k,k-1} &= \delta_{k \mod M}\\
        C_{k,k-M} &= \gamma_{k \mod M}
    \end{cases}
\end{align}
%
where 
%
\begin{align} 
    \begin{cases} 
        \beta_j &= 1 - v_j \frac{\Delta t}{\Delta x} + a'(v_j)\Delta t + a(v_j)\frac{\Delta t}{\Delta v}\\
        \delta_j &= - a(v_j)\frac{\Delta t}{\Delta v}\\
        \gamma_j &= v_j \frac{\Delta t}{\Delta x}
    \end{cases}
\end{align}
%

We note that the rows (and columns) of $\vecbf{B}$ and $\vecbf{C}$ are labelled in the same order as the rows of $\vecbf{u}^n$, namely $(1,1),(1,2),\dots,(1,M),(2,1),\dots(M,M)$. We map this labeling to the index $k$ running from $1$ to $M^2$ as 
%
\begin{align}
    k(i,j) = (i-1)M + j.
\end{align}
%
We note that for a fixed $j$ the coefficients $\beta_j$, $\gamma_j$, $\delta_j$ occur on rows labeled $(i,j)$ for $i=1,\dots,M$, and so on row $k$ of the matrix $C$, the coefficients labelled $j = k \mod M$ occur.

By inverting the matrix $\vecbf{B}$ in \autoref{eq:fd-matrix} we get an explicit expression for $\vecbf{u}^n$ in terms of $\vecbf{u}^{n-1}$, namely
%
\begin{align} 
    \label{eq:fd-matrix-ii}
    \vecbf{u}^n = \vecbf{B}^{-1}\vecbf{C}\vecbf{u}^{n-1} + \Delta t \vecbf{B}^{-1}\vecbf{z}^n.
\end{align}
%
This equation gives the procedure for stepping one step forward in time, so to get from $t=0$ to $T=0$, taking $n$ steps of size $\Delta t = \frac{T}{N}$ in time, we will need to apply the equation $N$ times. We also note that the scheme in \autoref*{eq:fd-matrix-ii} is only well-defined if $\vecbf{B}$ is invertible. Since $\vecbf{B}$ is tridiagonal, it is invertible if it is strictly diagonally dominant, i.e. if
%
\begin{align} 
    \sum_{j\neq i} \lvert B_{i,j} \rvert < \lvert B_{i,i} \rvert,  
\end{align}
%
which corresponds to the condition 
%
\begin{align}
    \sigma^2 \frac{\Delta t}{(\Delta v)^2} < 1 + \sigma^2 \frac{\Delta t}{(\Delta v)^2}.
\end{align}
%

\subsection{Monte Carlo methods for dual equation}

Now, we go on to discuss the details of the scheme implemented to solve the dual equation for $z$. As previously outlined, we will employ a Feynman-Kac formula connecting the dual equation to a stochastic differential equation (SDE), and then use Monte-Carlo simulation to obtain an approximation of the solution $z$.

Considering the dual equation in \autoref{eq:coupled-pdes}, we see that it can be written as
%
\begin{align} 
    \label{eq:dual-eqn}
    &\partial_t z + \mathcal{L} z + d_T - u = 0 \\
    &z(T,x,v) = 0
\end{align}
%
with the differential operator $\mathcal{L}$ being of the form
%
\begin{align} 
    \mathcal{L}z &= \vecbf{b}\cdot \nabla z + \div{\vecbf{A}\nabla z} \\
    &= v \partial_x z - a(v) \partial_v z + \frac{\sigma^2}{2}\partial^2_v z.
\end{align}
%
This is the generator of the stochastic process given by the two-dimensional SDE
%
\begin{align} 
    Y_t &= \begin{pmatrix}
        x\\
        v
    \end{pmatrix} 
    + \int_0^t \vecbf{b} ds + \int_0^t \sqrt{2}\vecbf{A}^{1/2} dW_s,\\
\end{align}
%
where $\vecbf{A}^{1/2}$ denotes the matrix square-root of $\vecbf{A}$. We also note that $W_s$ is a two-dimensional Brownian motion. Making explicit the forms of $\vecbf{A}$ and $\mathbf{b}$ we can write the SDE as

%
\begin{align}
    \label{eq:model-sde}
    Y_t = 
\begin{pmatrix}
    X_t\\
    V_t
\end{pmatrix}
= 
\begin{pmatrix}
    x\\
    v
\end{pmatrix}
+ \int_0^t 
\begin{pmatrix}
    V_s\\
    -a(V_s)
\end{pmatrix}
ds 
+ \int_0^t
\begin{pmatrix}
    0 & 0\\
    0 & \sigma 
\end{pmatrix}
\begin{pmatrix}
    dW_s^{(1)}\\
    dW_s^{(2)}
\end{pmatrix}.
\end{align}
%
Using the process $Y_t$, we can now write down a probabilistic representation of the solution to the dual equation \autoref{eq:dual-eqn} as
%
\begin{align}
    z(t,x,v) = \mathbb{E} \bigg[\int_t^{\tau^{t,x}\wedge T} d_T(s,X_s^{t,x},V_s^{t,x}) - u(s,X_s^{t,x},V_s^{t,x}) ds \bigg],
\end{align}
%
where $X_s^{t,x}$ denotes the stochastic process $X_s$ at time $s$, started at the initial value $x$ at time $t<s$. As before, $\tau^{t,x}$ denotes the first exit time from $\Omega$ of the process $X_s^{t,x}$.

To compute an approximation of $z$ using this probabilistic representation, we need to estimate the expectation $\mathbb{E}[(\cdot)]$. This is easily done by taking a Monte Carlo approach, where we estimate the expectation as the empirical mean over a number of sample paths $M$. Specifically,
%
\begin{align}
    \label{eq:dual-mc-approx}
    z(t,x,v) \approx \sum_{m=1}^{M} \int_t^{\tau^{t,x}\wedge T} d_T(s,X_s^{(m),t,x},V_s^{(m),t,x}) - u(s,X_s^{(m),t,x},V_s^{(m),t,x}) ds . 
\end{align}
%
This approximation converges at a rate proportional to $\frac{1}{\sqrt{M}}$, by the law of large numbers. 

As we cannot (in general) write down a closed-form expression for the solution to the SDE \autoref{eq:model-sde}, we also need to formulate a discretised approximation of the solution, which we can use to simulate the sample paths needed for the approximation of the expectation. For example, the Euler-Maruyama scheme for \autoref{eq:model-sde} is given by
%
\begin{align}
    X_{n+1} &= X_n + V_n \Delta t \\
    V_{n+1} &= V_n - a(V_n)\Delta t + \sigma \Delta W_n \\
    X_0 &= x, V_0 = v,
\end{align}
%
where we define the constant time step $\Delta t = T/N$, and $W_n$ denotes the Brownian increment
%
\begin{align} 
    W_n := W((n+1)\Delta t) - W(n\Delta t) \sim \mathcal{N}(0,\Delta t).
\end{align}
%

Finally, when applying these discretisation schemes in an implementation of the iterative algorithm proposed above, we need to consider how to evaluate the source terms and initial/terminal conditions. When solving \autoref{eq:primal-iteration} to obtain $u^{(k)}$ we simply need to know the values of $z^{(k-1)}$ at grid points. By using the same grid for solving the primal and dual equations, and storing the solution $z^{(k-1)}$, we can be sure to already have all the neccessary values.

On the other hand, when solving \autoref{eq:dual-iteration} using a Monte Carlo approach, we need to evaluate $u^{(k)}(t,x,v)$ at arbitrary points $(x,v)$, since we evaluate at the random points $(X_s^{(t,x)},V_s^{(t,v)})$. To evaluate $u^{(k)}$ between grid points in $x$ and $v$, we will simply linearly interpolate between the values at grid points.
We note that it is enough to be able to evaluate $u^{(k)}$ at grid points (rather than arbitrary points) in $t$, since the time variable remains deterministic. By choosing the same time discretisation for the dual equation as for the primal, we can ensure that the required points in time align with the grid points of the stored solution $u^{(k)}$. 

Finally, we also need to compute the integral with respect to time in \autoref{eq:dual-mc-approx}. For simplicity, we divide this into two integrals, both of which we compute using a left Riemann sum. First,
%
\begin{align}
    \int_{t_i}^{\tau^{t,x}\wedge T} u^{(k)}\big(s,X_s^{t,x},V_s^{t,v}\big) ds \approx \sum_{j=i}^{\min{(k_i,N-1)}} \Delta t u^{(k)}\big(t_j,X_{t_j}^{t_i,x},V_{t_j}^{t_i,x}\big),
\end{align}
% 
where $k_i$ denotes the index of the gridpoint corresponding to $\tau^{t,x}$. We note that it is only neccessary to compute the integral starting from a gridpoint $t_i$, since we have chosen the same time discretisation for the primal and dual equations. We also note that by making this choice, no additional error is introduced by replacing the integral by a Riemann sum, other than the potential error arising from assuming that $\tau^{t,x}$ lies on the time discretisation grid. In fact, the sum is exactly equal to the integral defined by $u^{(k)}$ on grid points, and linearly interpolated between them. 

Second we have
%
\begin{align}
    \int_{t_i}^{\tau{t,x}\wedge T} d_T(s,X_s^{t,x},V_s^{t,v}) ds \approx \sum_{j=i}^{\min{(k_i,N-1)}} \Delta t d_T\big(t_j,X_{t_j}^{t_i,x},V_{t_j}^{t_i,v}\big).
\end{align}
%
In this case, the Riemann sum is an approximation of the integral, since the function $d_T(t,x,v)$ is the target dose, defined without interpolation.

%

\subsection{Numerical schemes for FBSDEs}\label{sec:fbsde-numerics}
In this section, for completeness, we present some numerical methods for the simulation of solutions to the FBSDE described in \autoref{sec:fbsde-theory}. These numerical methods have not been implemented in the work behind this report, but are rather presented for the purposes of discussion of future work.

To solve the FBSDE \autoref{eq:fbsde} numerically, we need to discretise both the forward and the backward SDE in time. Discretising the forward SDE is straightforward, and can for example be done using the (forward) Euler-Maruyama scheme
%
\begin{align} 
    \begin{cases}
    &X_0^{h} = x\\
    &X_{(i+1)h}^{(h)} = X_{ih}^{(h)} + b(ih,X_{ih}^{(h)}) h + \sigma(ih,X_{ih}^{(h)})(W_{(i+1)h}-W_{ih}).
    \end{cases}
\end{align}
%
We will use this discretised version of $X_t$ in the scheme for $Y_t=u(t,X_t)$, or similarly in the scheme for $u(t,x)$. For simplicity, we will here consider the case where $g$ is a function of $t,x,$ and $u$, but not of $\nabla u$.

By using the tower property of expectation on \autoref{eq:nonlinear-fc-for-y}, we can express $Y_{t_i}$ in terms of $Y_{t_{i+1}}$, and in terms of $X_t$ between $t_i$ and $t_{i+1}$ as
%
\begin{align} 
    Y_{t_i} = \mathbb{E}\bigg[ Y_{t_{i+1}} + \int_{t_{i}}^{t_{i+1}}g(s,X_s,Y_s)ds \Big\lvert X_{t_i} \bigg].
\end{align}
%
We recall that we are solving for $Y_t$ backwards in time, so for $t_i < t_{i+1}$, $Y_{t_{i+1}}$ is known.

From the above, we can get the Euler scheme (iterating backwards in time) for $Y$, namely
%
\begin{align} % not entirely sure we're conditioning on the right thing here? But seems consistent with the below...
    \begin{cases}
    &Y_T^{(h)} = f(X_T^{h})\\
    &Y_{ih}^{(h)} = \mathbb{E}\bigg[ Y_{(i+1)h}^{(h)} + h g(t_i,X_{ih}^{(h)},Y_{(i+1)h}^{(h)}) \Big\lvert X_{t_i}^{(h)} \bigg].
    \end{cases}
\end{align}
%

By conditioning on the discretised process $X^{(h)}$ starting at a specific value $x$ at time $t_i$, we get the same scheme for $u(t,x)$:
%
\begin{align}
    \begin{cases}
    &u^{(h)}(T,x) = f(x)\\
    &u^{(h)}(t_i,x) = \mathbb{E}\bigg[ u^{(h)}(t_{i+1},X^{(h),t_i,x}_{t_{i+1}}) + h g\big( t_i,x,u^{(h)}(t_{i+1},X^{(h),t_i,x}_{t_{i+1}}) \big) \bigg]. 
    \end{cases}
\end{align} 
%

We note here that the accuracy of a numerical simulation of $u(t,x)$ (or $Y_t$) depends on both the scheme we choose for $X_t$, and the scheme we choose for $Y_t$. In \cite{fang2023strong} a strong stability preserving multistep scheme, which improves the latter, is introduced. This scheme is given by
%
\begin{align} % Is this in fact right? look at X-term
    Y_{ih}^{(h)} = \sum_{j=1}^{k} \alpha_j \mathbb{E}\bigg[ Y_{(i+j)h}^{(h)} \Big\lvert X_{t_i}^{(h)} \bigg] + h \sum_{j=1}^{k} \beta_j \mathbb{E}\bigg[ g\big(t_{i+j},X_{(i+j)h}^{(h)},Y_{(i+j)h}^{(h)}\big) \Big\lvert X_{t_i}^{(h)} \bigg].
\end{align}
%
The corresponding scheme for $u(t,x)$ becomes
%
\begin{align} 
    u^{(h)}(t_i,x) &= \sum_{j=1}^{k} \alpha_j \mathbb{E}\bigg[ u^{(h)}\Big(t_{i+j},X_{t_{i+j}}^{(h),t_i,x}\Big) \bigg]\\
    &+ h \sum_{j=1}^{k} \beta_j \mathbb{E}\bigg[ g\Big(t_{i+j},X_{t_{i+j}}^{(h),t_i,x},u^{(h)}\big(t_{i+j},X_{t_{i+j}}^{(h),t_i,x}\big)\Big) \bigg].
\end{align}
%

